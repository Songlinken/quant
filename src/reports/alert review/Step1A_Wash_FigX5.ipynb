{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wash Sales Case Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sqian/.pyenv/versions/3.7.0/envs/env_370/lib/python3.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import os; \n",
    "import pandas as pd; pd.set_option('mode.chained_assignment',None) \n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import math\n",
    "import re\n",
    "import xlrd\n",
    "from xlsxwriter.utility import xl_rowcol_to_cell\n",
    "from random import sample\n",
    "from src.data_models.SmartsCsvDataModel import SmartsCsvDataModel\n",
    "from src.data_models.smartshelper import metrics, fixNum, unSMART, cparty, totalUP, deets, wtf, identities\n",
    "from src.data_models.SmartsDataModel import SmartsDataModel\n",
    "from src.utility.DataModelUtility import execute_query_data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Set Alerting Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alerting_1 = datetime.date(2019,8,1)\n",
    "alerting_2 = datetime.date(2019,8,31)\n",
    "\n",
    "os.chdir('/Users/sqian/Documents/{}_output'.format(alerting_1.strftime(\"%Y%m\")))\n",
    "plt.rcParams['figure.figsize'] = (12,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Import CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> type fixed, kept 179 rows, 12 columns\n"
     ]
    }
   ],
   "source": [
    "rawalerts = unSMART(pd.read_excel('~/Documents/vmfldr/smarts1_{}.xlsx'.format(alerting_1.strftime(\"%Y%b\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Prep - Alert Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "washDateAlert = rawalerts.loc[(rawalerts['AlertCode'] == 4042) | (rawalerts['AlertCode'] == 4041),['AlertCode','Date']].reset_index(drop=True)\n",
    "# washDateAlert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cases Excluding Wash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "codelist = rawalerts.groupby('AlertCode').count().index.tolist()\n",
    "fig_0 = []\n",
    "\n",
    "for i in codelist:\n",
    "    data = metrics(rawalerts.loc[rawalerts['AlertCode'] == i,'AccountIDName']).index.tolist()\n",
    "    datalist = [i,len(data),data]\n",
    "    fig_0.append(datalist)\n",
    "    \n",
    "fig_0 = pd.DataFrame(fig_0).rename(columns={0:'code',1:'countAccounts',2:'accList'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Prep - Alert Event Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pd.DataFrame()\n",
    "\n",
    "for i in range(0,washDateAlert.shape[0]):\n",
    "    temp = pd.read_csv('~/Documents/vmfldr/washtext{}/All_Securities_{}_{}.csv'.format(washDateAlert.loc[i,'Date'].strftime(\"%Y%m\"),washDateAlert.loc[i,'AlertCode'],washDateAlert.loc[i,'Date'].strftime(\"%Y%m%d\")))\n",
    "    temp['AlertCode'] = washDateAlert.loc[i,'AlertCode']\n",
    "    newdf = newdf.append(temp,sort=False)\n",
    "\n",
    "newdf.columns = newdf.columns.str.replace(' ','')\n",
    "newdf['datetime'] = pd.to_datetime(newdf['Date'],format=' %d/%m/%Y')\n",
    "newdf['month'] = pd.to_datetime(newdf['datetime']).dt.month\n",
    "newdf['year'] = pd.to_datetime(newdf['datetime']).dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure A: Wash Alerts Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "washalerts = rawalerts.loc[(rawalerts['AlertCode'] == 4041) | (rawalerts['AlertCode'] == 4042)]\n",
    "\n",
    "fig_A_alerts = pd.pivot_table(washalerts.groupby(['year','month','AlertCode']).count(), values='Datetime', index=['year','month'], columns=['AlertCode'], aggfunc=np.sum).fillna(value=0).astype(int)\n",
    "fig_A_alertsT = fig_A_alerts.sum(axis=1)\n",
    "fig_A_events = pd.DataFrame(newdf.groupby(['year','month']).count()['datetime'])\n",
    "fig_A_draft = pd.concat([fig_A_alerts, fig_A_alertsT, fig_A_events],axis=1).rename(columns={0:'alerts','datetime':'events'})\n",
    "\n",
    "fig_A = pd.concat([fig_A_draft, pd.DataFrame(fig_A_draft.sum(axis=0)).rename(columns={0:'TOTAL'}).transpose()],axis=0)\n",
    "fig_A['events/alert'] = round(fig_A['events']/fig_A['alerts'],1)\n",
    "# fig_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure B: Alert Event Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = [alerting_1 + datetime.timedelta(days=x) for x in range(0, 30)]\n",
    "datePD = pd.DataFrame(date_list).set_index(0)\n",
    "data = pd.DataFrame(newdf['datetime'].dt.date.value_counts())\n",
    "\n",
    "plotdata = pd.concat([datePD,data['datetime']],axis=1,sort=True).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sqian/.pyenv/versions/3.7.0/envs/env_370/lib/python3.7/site-packages/pandas/plotting/_converter.py:129: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n",
      "\n",
      "To register the converters:\n",
      "\t>>> from pandas.plotting import register_matplotlib_converters\n",
      "\t>>> register_matplotlib_converters()\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "plt.plot_date(x=plotdata.index, y=plotdata['datetime'], fmt=\"go-\",xdate=True)\n",
    "plt.ylabel(\"Number of Events\")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=20)\n",
    "plt.title(\"{} - {}: Wash Sale Alert Events\".format(alerting_1.strftime(\"%Y/%m/%d\"),alerting_2.strftime(\"%Y/%m/%d\")))\n",
    "plt.savefig(\"{}_Wash_fig_B.png\".format(alerting_1.strftime(\"%Y%b\")))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure C: Alert Events by Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf2 = newdf[['BuyerName','Security','datetime']].rename(columns={'BuyerName':'acc'}).append(newdf[['SellerName','Security','datetime']].rename(columns={'SellerName':'acc'}))\n",
    "fig_Cdraft = pd.DataFrame(newdf2['acc'].value_counts())\n",
    "fig_C = pd.concat([fig_Cdraft[:10],\n",
    "                   pd.DataFrame(fig_Cdraft[10:].sum()).rename(columns={0:'remaining'}).transpose(),\n",
    "                   pd.DataFrame(fig_Cdraft.sum()).rename(columns={0:'TOTAL'}).transpose()],axis=0).rename(columns={'acc':'count'})\n",
    "fig_C['percAcc'] = round(fig_C['count']/fig_Cdraft['acc'].sum()*100).astype(int).astype(str) + '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure D: Alert Events by Account Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf['acc1'] = newdf[['BuyerName','SellerName']].min(axis=1)\n",
    "newdf['acc2'] = newdf[['BuyerName','SellerName']].max(axis=1)\n",
    "\n",
    "newdf['pair_POV1'] = newdf['acc1'].astype(str) + ' & ' + newdf['acc2'].astype(str)\n",
    "newdf['passPOV'] = (2 * (newdf['acc1'] == newdf['BuyerName'])-1) * newdf['MoneyPassAmount']\n",
    "newdf = newdf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_D_draft = pd.DataFrame(newdf['pair_POV1'].value_counts())\n",
    "fig_D = pd.concat([fig_D_draft[:10],\n",
    "                   pd.DataFrame(fig_D_draft[10:].sum()).rename(columns={0:'remaining'}).transpose(),\n",
    "                   pd.DataFrame(fig_D_draft.sum()).rename(columns={0:'TOTAL'}).transpose()],axis=0).rename(columns={'pair_POV1':'count'})\n",
    "fig_D['percAcc'] = round(fig_D['count']/fig_D_draft['pair_POV1'].sum()*100).astype(int).astype(str) + '%'\n",
    "# fig_D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Grouping >>> Identifying Worst Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay:\n",
      "[1507866, 772544, 444259, 711292, 355845, 952523, 453659, 3032, 2647, 395052, 1595, 898579, 450782, 135552, 195483, 568127, 885724, 1403, 1169466]\n",
      "\n",
      "worst:\n",
      "[22440, 811415]\n"
     ]
    }
   ],
   "source": [
    "smallestcat = 0\n",
    "percentageAlerts = 0.10\n",
    "\n",
    "while smallestcat < 0.05:\n",
    "    percentageAlerts = percentageAlerts + 0.01\n",
    "    \n",
    "    washMetrics = pd.DataFrame(newdf2['acc'].value_counts()/newdf2['acc'].value_counts().sum())\n",
    "    worst = washMetrics.loc[washMetrics['acc'] >= percentageAlerts].index.tolist()\n",
    "    okay = washMetrics.loc[washMetrics['acc'] < percentageAlerts].index.tolist()\n",
    "\n",
    "\n",
    "    worstaccSorted = sorted(worst, reverse=False)\n",
    "    acclist = []\n",
    "    for i in worstaccSorted:\n",
    "        for j in worstaccSorted:\n",
    "            if (i <= j):\n",
    "                if (i == j): temp = str(i) + ' & all'\n",
    "                elif (i in worstaccSorted) & (j in worstaccSorted): temp = str(i) + ' & ' + str(j)\n",
    "                acclist.append(temp)\n",
    "\n",
    "    newdf['CaseDesc'] = ''\n",
    "    newdf['acc1str'] = newdf['acc1'].astype(str) + ' & all'\n",
    "    newdf['acc2str'] = newdf['acc2'].astype(str) + ' & all'\n",
    "    newdf['TV'] = newdf['Trade1Value'] + newdf['Trade2Value']\n",
    "\n",
    "    for i in range(0,newdf.shape[0]):\n",
    "        if newdf.loc[i,'pair_POV1'] in acclist: newdf.loc[i,'CaseDesc'] = newdf.loc[i,'pair_POV1']\n",
    "        elif (newdf.loc[i,'acc1str'] in acclist): newdf.loc[i,'CaseDesc'] = newdf.loc[i,'acc1str']\n",
    "        elif (newdf.loc[i,'acc2str'] in acclist): newdf.loc[i,'CaseDesc'] = newdf.loc[i,'acc2str']\n",
    "        else: newdf.loc[i,'CaseDesc'] = 'other than ' + str(worst)\n",
    "\n",
    "\n",
    "    output1 = newdf.groupby(['CaseDesc']).count()['Date'].reset_index()\n",
    "    output1[['split1','split2']] = output1['CaseDesc'].str.split('&',expand=True)\n",
    "    output1.loc[(output1['split2'] == 'all'),'split2'] = 999999\n",
    "    output1.loc[output1['split1'] == ('other than ' + str(worst)),'split1'] = 999999\n",
    "    output1['split1'] = output1['split1'].astype(int)\n",
    "    output1 = output1.sort_values('split1').sort_values('split2').reset_index(drop=True).reset_index(drop=False).drop(columns=['split1','split2'])\n",
    "    output1['Case'] = 'Case ' + (output1['index']+1).apply(lambda x: '{0:0>2}'.format(x))\n",
    "    output2 = pd.concat([output1[['Case','CaseDesc','Date']].rename(columns={'Date':'events'}),\n",
    "                         pd.DataFrame(output1[['Date']].rename(columns={'Date':'events'}).sum(axis=0)).transpose()],axis=0,sort=False)\n",
    "    output2['percAcc0'] = output2['events']/output1['Date'].sum()\n",
    "    output2['percAcc'] = round(output2['percAcc0']*100).astype(int).astype(str) + '%'\n",
    "    output2.loc[output2['Case'].isna(),['Case','CaseDesc']] = 'TOTAL'\n",
    "\n",
    "    smallestcat = output2['percAcc0'].min()\n",
    "    \n",
    "print('okay:', okay, '\\nworst:', worst,sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Grouping >>> Creating Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "casemapping = output2.set_index('CaseDesc')['Case'].to_dict()\n",
    "\n",
    "newdf['CaseNo'] = newdf['CaseDesc'].map(casemapping)\n",
    "newdf['AlertID'] = 'Wash Sale ABA (' + (newdf['AlertCode']).astype(str) + ') | ' + newdf['datetime'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "eventbd = pd.DataFrame(newdf.groupby(['CaseNo','CaseDesc']).count()['Security'])\n",
    "eventbd['pct'] = round(eventbd['Security']/eventbd['Security'].sum()*100).astype(int).astype(str)+'%'\n",
    "uAlerts = newdf.groupby(['CaseNo','AlertCode','CaseDesc'])['Date'].apply(pd.unique).apply(len).reset_index().groupby(['CaseNo','CaseDesc']).sum()['Date']\n",
    "uPair = newdf.groupby(['CaseNo','CaseDesc'])['pair_POV1'].apply(pd.unique).apply(len)\n",
    "uAcc_draft = newdf[['CaseNo','CaseDesc','acc1']].append(newdf[['CaseNo','CaseDesc','acc2']].rename(columns={'acc2':'acc1'}),sort=True)\n",
    "uAcc = uAcc_draft.groupby(['CaseNo','CaseDesc'])['acc1'].apply(pd.unique).apply(len)\n",
    "uSUM = round(newdf.groupby(['CaseNo','CaseDesc']).sum()['TV']/1000).astype(int)\n",
    "\n",
    "datafix = pd.concat([newdf.groupby(['CaseNo','CaseDesc'])['datetime'].max(),newdf.groupby(['CaseNo','CaseDesc'])['datetime'].min(),newdf.groupby(['CaseNo','CaseDesc'])['datetime'].apply(pd.unique).apply(len)],axis=1)\n",
    "datafix.columns=['max','min','unique']\n",
    "datafix['span'] = datafix['max'] - datafix['min']\n",
    "datafix = datafix.reset_index()\n",
    "ltext1 = ' includes Wash Sale Alert Events triggered on activity between Accounts '\n",
    "datafix['ltext'] = datafix['CaseNo'] + ltext1 + datafix['CaseDesc'] + ' between {} and {} on SMARTS Calibrated Version 1.'.format(alerting_1.strftime(\"%B %d, %Y\"), alerting_2.strftime(\"%B %d, %Y\"))\n",
    "datafix = datafix.set_index(['CaseNo','CaseDesc'])\n",
    "datafix['days'] = newdf.groupby(['CaseNo','CaseDesc'])['Date'].apply(pd.unique).apply(len)\n",
    "datafix['ltext2'] = 'The alerts in this case trigger on ' + datafix['days'].astype(str) + ' days, spanning '+ datafix['span'].astype(str).str.slice(start=0, stop=7, step=1) + ' total days.'\n",
    "\n",
    "renamecols = {'pair_POV1':'#pairs','pair_POV2':'CaseDesc','Date':'#alerts','index':'Case','Security':'#events','acc1':'#accs','TV':'washed$000'}\n",
    "fig_E_draft = pd.concat([eventbd,uAlerts,uPair,uAcc,uSUM,datafix[['ltext','ltext2']]],axis=1).rename(columns=renamecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "caseotherlist = pd.unique(pd.melt(newdf.loc[newdf['CaseNo'] == 'Case 04',['acc1', 'acc2']].reset_index(drop=True).reset_index(), id_vars='index', value_vars=['acc1', 'acc2'])['value']).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventbd = newdf.count()['Security']; eventbd\n",
    "pcttotal = '100%'\n",
    "\n",
    "uAlerts = newdf.groupby(['AlertCode','Date']).count().shape[0]; uAlerts\n",
    "uPair = len(pd.unique(newdf['pair_POV1'])); uPair\n",
    "uAcc_draft = newdf[['CaseNo','CaseDesc','acc1']].append(newdf[['CaseNo','CaseDesc','acc2']].rename(columns={'acc2':'acc1'}),sort=True)\n",
    "uAcc = len(pd.unique(uAcc_draft['acc1'])); uAcc\n",
    "uSUM = round(newdf.sum()['TV']/1000).astype(int); uSUM\n",
    "\n",
    "fig_E_total = pd.DataFrame(['TOTAL', eventbd, pcttotal, uAlerts,uPair,uAcc,uSUM]).transpose().rename(columns={0:'CaseNo'}).set_index('CaseNo')\n",
    "fig_E_total.columns = fig_E_draft.columns[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_E = pd.concat([fig_E_draft.reset_index(), fig_E_total.reset_index()],axis=0,sort=False)\n",
    "fig_E = fig_E[['CaseNo', 'CaseDesc', '#events', 'pct', '#alerts', '#pairs', '#accs', 'washed$000','ltext','ltext2']]\n",
    "fig_E['washed$000'] = fig_E['washed$000'].map('{:,.0f}'.format)\n",
    "# fig_E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_F = fig_E.reset_index()[['CaseNo','#alerts']]\n",
    "# fig_F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figures 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output= newdf\n",
    "\n",
    "for i in fig_E.reset_index()['CaseNo'][:-1]:\n",
    "    tempdfdata = pd.DataFrame(newdf.loc[newdf['CaseNo'] == i,'datetime'].dt.date.value_counts())\n",
    "    plotdata = pd.concat([datePD,tempdfdata['datetime']],axis=1,sort=True).fillna(0).astype(int)\n",
    "    numba = fig_F.loc[fig_F['CaseNo'] == i].reset_index()['index'][0] + 1\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = (12,4)\n",
    "    plt.plot_date(x=plotdata.index, y=plotdata['datetime'], fmt=\"go-\",xdate=True)\n",
    "    plt.title(i+' Alert Events')\n",
    "    plt.ylabel(\"Events\")\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=20)\n",
    "    plt.savefig(\"{}_{}_fig_{}.1.png\".format(alerting_1.strftime(\"%Y%b\"),i,numba))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEBUGGING HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acclist = pd.unique(np.concatenate(newdf.groupby('CaseNo')['Security'].apply(pd.unique)))\n",
    "\n",
    "# idee = identities(acclist,alerting_1,alerting_2)\n",
    "# for i in range(1,len(idee)): print(idee.iloc[i]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figures 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalsub = fig_E.reset_index()[['CaseNo','#events','#pairs']]\n",
    "\n",
    "fig_X = pd.concat([newdf.groupby(['CaseNo','Date']).count()['Description'],\n",
    "                   newdf.groupby(['CaseNo','Date'])['pair_POV1'].apply(pd.unique).apply(len)],\n",
    "                  axis=1, sort=False).rename(columns={'Description':'Events'}).reset_index(drop=False)\n",
    "\n",
    "casepairmap = newdf.groupby('CaseNo')['pair_POV1'].apply(pd.unique).apply(len).to_dict()\n",
    "\n",
    "fig_X['totalunique'] = fig_X['CaseNo'].map(casepairmap)\n",
    "fig_X['CaseName'] = 'Wash Sale ABA |' + fig_X['Date']\n",
    "\n",
    "fig_X2 = {}\n",
    "for i in fig_F['CaseNo'][:-1]: \n",
    "    temp = fig_X.loc[fig_X['CaseNo'] == i,['CaseName','Events','pair_POV1']].sort_values('Events',ascending=False).rename(columns={'CaseName':'CaseNo','Events':'events','pair_POV1':'#pairs'})\n",
    "    temp1 = pd.DataFrame(temp[10:].sum(axis=0)).transpose()\n",
    "    temp1['CaseNo'] = 'remaining'\n",
    "    temp2 = pd.DataFrame(temp.sum(axis=0)).transpose()\n",
    "    temp2['CaseNo'] = 'TOTAL'\n",
    "    fig_X2[i] = pd.concat([temp[:10], temp1, temp2], axis=0, sort=False).reset_index(drop=True)\n",
    "    \n",
    "# fig_X2.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figures 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_X3_a = pd.DataFrame(output.groupby(['CaseNo','Security','pair_POV1']).count()['Date'])\n",
    "fig_X3 = pd.pivot_table(fig_X3_a, values='Date', index=['CaseNo','pair_POV1'], columns=['Security'], aggfunc=np.sum).fillna(value=0).astype(int).reset_index().set_index('pair_POV1')\n",
    "fig_X3['TOTAL'] = fig_X3.drop(columns='CaseNo').sum(axis=1)\n",
    "# fig_X3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradingpairsoi = output['Security'].value_counts().index.tolist()\n",
    "# tradingpairsoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_X3dict = {}\n",
    "\n",
    "for i in fig_X3.groupby('CaseNo')['CaseNo'].count().index.tolist():\n",
    "    tempdf = fig_X3.loc[fig_X3['CaseNo'] == i].sort_values('TOTAL', ascending=False)\n",
    "    tempdf2 = pd.concat([tempdf[:10],\n",
    "                         pd.DataFrame(tempdf[10:].sum(axis=0)).rename(columns={0:'remaining'}).transpose(),\n",
    "                         pd.DataFrame(tempdf.sum(axis=0)).rename(columns={0:'TOTAL'}).transpose()], axis=0)\n",
    "    tempdf2.loc[tempdf2['CaseNo'] != i,'CaseNo'] = '-'\n",
    "    tempdf3 = tempdf2.drop(columns='CaseNo').reset_index(drop=False).rename(columns={'index':'Account Pairs'})\n",
    "    fig_X3dict[i] = tempdf3.loc[tempdf3['TOTAL'] > 0]\n",
    "\n",
    "# fig_X3dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figures 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_X4 = pd.concat([output.groupby('pair_POV1')['CaseNo'].max(),\n",
    "                    metrics(output['pair_POV1']),\n",
    "                    output.groupby('pair_POV1').sum().astype(int)[['passPOV','TV']].sort_values('TV',ascending=False)],\n",
    "                   axis=1,sort=False)\n",
    "fig_X4['passperc'] = round(fig_X4['passPOV']/fig_X4['TV']*100,1).astype(str) + '%'\n",
    "fig_X4['TV000'] = round(fig_X4['TV']/1000)\n",
    "\n",
    "fig_X4 = fig_X4.sort_values('CaseNo').reset_index()\n",
    "fig_X4[['party1','party2']] = fig_X4['index'].str.split(' & ',expand=True)\n",
    "# fig_X4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "listt = fig_X4[['CaseNo','party1']].rename(columns={'party1':'party2'})\n",
    "acccasemap = listt.groupby('CaseNo')['party2'].apply(lambda x: x.value_counts().index[0]).to_dict()\n",
    "\n",
    "fig_X4['Case_mainacc'] = fig_X4['CaseNo'].map(acccasemap)\n",
    "fig_X4['bool'] = 2 * (fig_X4['Case_mainacc'] == fig_X4['party1']) - 1\n",
    "fig_X4['passAdj'] = fig_X4['passPOV'] * fig_X4['bool']\n",
    "fig_X4['adjindex'] = fig_X4['party2'] + ' & ' + fig_X4['party1']\n",
    "\n",
    "def func(row):\n",
    "    if row['bool'] == -1: return row['adjindex']\n",
    "    else: return row['index']\n",
    "\n",
    "fig_X4['combo'] = fig_X4.apply(func, axis=1)\n",
    "fig_X4final = fig_X4[['CaseNo','combo','count','passAdj','TV000']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_X4dict = {}\n",
    "\n",
    "for i in fig_X4final.groupby('CaseNo')['CaseNo'].count().index.tolist():  \n",
    "    part0 = fig_X4final.loc[fig_X4final['CaseNo'] == i].sort_values('count', ascending=False)\n",
    "    part00 = pd.DataFrame(part0[10:].sum(axis=0)).transpose()\n",
    "    part00[['CaseNo','combo']] = 'remaining'\n",
    "    \n",
    "    part1 = part0[:10].append(part00, sort=True).reset_index(drop=True)\n",
    "    part2 = pd.DataFrame(part1.sum(axis=0)).transpose()\n",
    "    part2[['CaseNo','combo']] = 'TOTAL'\n",
    "    \n",
    "    final = pd.concat([part1,part2],axis=0)\n",
    "    final = final[(final['TV000'] != 0)]\n",
    "    \n",
    "    final['%passed'] = round((final['passAdj']/(final['TV000']*1000)*100).astype(float),1).astype(str) + '%'\n",
    "    fig_X4dict[i] = final[['CaseNo','combo','count','passAdj','TV000' ,'%passed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figures 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.DataFrame(rawalerts.groupby(['Date','AlertCode','ShortText_mod','AccountIDName']).count()['Datetime'])\n",
    "\n",
    "fig_X7a = pd.pivot_table(data, values='Datetime', index=['AlertCode'], columns=['AccountIDName'], aggfunc=np.sum).fillna(value=0).astype(int).dropna(axis='columns',how='all')\n",
    "fig_X7a = pd.concat([fig_X7a,pd.DataFrame(fig_X7a.sum(axis=0)).transpose()],axis=0)\n",
    "fig_X7b = fig_X7a.drop(columns=[0])[worst]\n",
    "fig_X7b['TOTAL'] = fig_X7a.sum(axis=1)\n",
    "\n",
    "for i in worst:\n",
    "    fig_X7b[str(i)+'_%'] = round(fig_X7b[i] / fig_X7b['TOTAL']*100).fillna(0).astype(int).astype(str) + '%'\n",
    "fig_X7c = pd.concat([fig_X7b,data.reset_index()[['AlertCode','ShortText_mod']].drop_duplicates().set_index('AlertCode')], axis=1).reset_index().rename(columns={'index':'AlertCode'})\n",
    "fig_X7c.loc[fig_X7c['ShortText_mod'].isna(),'AlertCode'] = 99999\n",
    "\n",
    "fig_X7c.columns = fig_X7c.columns.astype(str)\n",
    "fig_X7 = fig_X7c[['AlertCode','ShortText_mod', '22440', '22440_%', '811415', '811415_%', 'TOTAL']].sort_values('AlertCode')\n",
    "# fig_X7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "otheralerts = data.reset_index().loc[~data.reset_index()['AlertCode'].isin([4040,4041,4042])]\n",
    "dataspoof = otheralerts.reset_index().loc[data.reset_index()['AlertCode'].isin([4022,4023,4032])]\n",
    "\n",
    "fig7_lastcase = pd.concat([otheralerts.groupby('AccountIDName')['Datetime'].sum(),dataspoof.groupby('AccountIDName')['Datetime'].sum()],axis=1).fillna(0).astype(int)\n",
    "fig7_lastcase.columns=['totalAlerts','spoofAlerts']\n",
    "fig7_lastcase2 = fig7_lastcase.sort_values('totalAlerts', ascending=False)\n",
    "fig7_lastcase2[fig7_lastcase2.columns + '_%'] = round(fig7_lastcase2/fig7_lastcase2.sum()*100,0).astype(int).astype(str)+'%'\n",
    "fig7_other = fig7_lastcase2\n",
    "# fig7_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_symbol = str(pd.unique(np.concatenate(newdf.groupby('CaseNo')['Security'].apply(pd.unique))).tolist()).replace('[','').replace(']','')\n",
    "\n",
    "# acclist = pd.concat([newdf['acc1'],newdf['acc2']], axis=0).value_counts()/pd.concat([newdf['acc1'],newdf['acc2']], axis=0).count()\n",
    "# list_account = str(acclist.loc[acclist >= 0.05].index.tolist()).replace('[','').replace(']',''); list_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query2 = \"\"\"\n",
    "#         SELECT *\n",
    "#         FROM ms_prod.smarts_raw_data\n",
    "#         WHERE event_type = 'Place' and symbol in ({sec}) and account_id in ({acc}) and event_date between '{t1}' and '{t2}' \n",
    "#         \"\"\".format(sec = list_symbol,acc = list_account,t1 = alerting_1,t2 = alerting_2)\n",
    "# new = execute_query_data_frame(query2, 'gemrdsdb', ssh = 'interim')\n",
    "# new.columns = new.columns.str.replace('_crypto','')\n",
    "# new['date_time'] = new['event_date'] + pd.to_timedelta(new['event_time'].astype(str)) + new['event_millis']\n",
    "\n",
    "# gb = new.groupby(['account_id','symbol','order_type','execution_options','side']).count()\n",
    "# table = pd.pivot_table(gb, columns='side', index=['account_id','symbol','order_type','execution_options'], values='event_id')\n",
    "# table1 = pd.concat([table,table.sum(axis=1)], axis=1).rename(columns={0:'total'}).sort_values('total',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wash Case Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writer = pd.ExcelWriter('{}_Wash.xlsx'.format(alerting_1.strftime(\"%Y%b\")), engine='xlsxwriter')\n",
    "\n",
    "identities(worst,alerting_1,alerting_2).to_excel(writer, 'id')\n",
    "\n",
    "fig_A.to_excel(writer, 'f.A')\n",
    "# fig_B is a PLOT\n",
    "fig_C.to_excel(writer, 'f.C')\n",
    "fig_D.to_excel(writer, 'f.D')\n",
    "fig_E.to_excel(writer, 'f.E')\n",
    "fig_F.to_excel(writer, 'f.F')\n",
    "\n",
    "# fig7\n",
    "fig_X7.to_excel(writer, 'f.7')  \n",
    "fig7_other.to_excel(writer, 'f.7+')\n",
    "\n",
    "# fig1 plots\n",
    "for i in fig_F['CaseNo'][:-1]: \n",
    "    fig_X2[i].to_excel(writer,'{}.{}'.format(i.replace('Case ','C'),'f2'))\n",
    "    fig_X3dict[i].to_excel(writer,'{}.{}'.format(i.replace('Case ','C'),'f3'))\n",
    "    fig_X4dict[i].to_excel(writer,'{}.{}'.format(i.replace('Case ','C'),'f4'))\n",
    "\n",
    "# Alert/Case mapping\n",
    "outputmap = []\n",
    "output[['Date','AlertID','CaseNo','pair_POV1','Description','Security','TV','passPOV']].to_excel(writer, 'EventMapping')\n",
    "\n",
    "for i in output.groupby('AlertID').sum().index:\n",
    "    toprint = output.loc[output['AlertID'] == i,'CaseNo'].value_counts().index.tolist()\n",
    "    outputmap.append([i,sorted(toprint)])\n",
    "pd.DataFrame(outputmap).to_excel(writer, 'AlertMapping')\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:paramiko.transport:Connected (version 2.0, client OpenSSH_7.4p1)\n",
      "INFO:paramiko.transport:Authentication (publickey) successful!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh_connect 0:00:00.621077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded dataframe with 2 rows in 0:00:00.879523.\n",
      "INFO:paramiko.transport:Connected (version 2.0, client OpenSSH_7.4p1)\n",
      "INFO:paramiko.transport:Authentication (publickey) successful!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh_connect 0:00:00.586097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded dataframe with 4 rows in 0:00:10.640640.\n"
     ]
    }
   ],
   "source": [
    "identities(worst,alerting_1,alerting_2).to_excel(writer, 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
